---
title: What Are We Even Doing?
layout: layouts/post.njk
subtitle: "or: How I Learned to Stop Worrying and Love Artificial Intelligence"
image: assets/images/what_are_we_even_doing/Screenshot 2025-10-08 at 18-05-46 ChatGPT.png
alt: "Screenshot of OpenAI's ChatGPT website on Desktop."
caption: "Screenshot of OpenAI's ChatGPT website on Desktop."
date: 2025-10-08
---

Whilst browsing YouTube, I came across [AI In Context](https://www.youtube.com/@AI_In_Context)'s '[We're Not Ready for Superintelligence](https://www.youtube.com/watch?v=5KVDDfAkRgc)'. This video, to me, was conspiratorial and made a lot of assumptions that just felt wrong. I mean just look at the description of the video:

> AI 2027 depicts a possible future where artificial intelligence radically transforms the world in just a few intense years. It's based on detailed expert forecasts — but how much of it will actually happen? Are we really racing towards a choice between a planet controlled by the elite, or one where humans have lost control entirely?
>
>My takeaway? Loss of control, racing scenarios, and concentration of power are all concerningly plausible, and among the most pressing issues the world faces. [^1]

<iframe src="https://www.youtube.com/embed/5KVDDfAkRgc" title="We&#39;re Not Ready for Superintelligence" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

So after watching that conspiratorial apocalyptic mess, I decided to do some digging, and this is what I found.

[Aric Floyd](https://www.imdb.com/name/nm5354308/), the host of the channel is an actor. The channel is not a personal project of some AI enthusiast. It was created by the nonprofit '[80,000 Hours](https://80000hours.org/)'.

You might recognize 80,000 Hours from a YouTube sponsor read. Here's one I found from Extra Credits:

> 80,000 Hours is a nonprofit organization dedicated to helping people find careers that are not only fulfilling but also have a significant positive impact on the world. Their work is based on over 10 years of research, conducted in collaboration with Oxford University, aimed at identifying high-impact career paths that can tackle some of humanity's biggest challenges.[^2]

So you might be wondering as to why a career training website would be producing an apocalyptic AI video about how humanity will need to fight against an all-powerful AI. The answer is that 80,000 Hours aligns itself with the philosophy of effective altruism.

> Effective Altruism is a form of utilitarianism, itself a branch of consequentialism – the plausible idea that you judge choices by their consequences. Utilitarians go further by saying you judge acts only by their consequences, and that the outcomes can be graded on a single scale of utility, or happiness. You judge acts by comparing their effects on the global sum of happiness, and the morally right course is the one you expect to achieve the maximum effect. Crucially, other moral considerations – obligations to be honest, to be just, to be loyal, to respect property rights and many more – count only to the extent that they bear on the happiness calculation. [^5]

I had first heard of effective altruism in 2022, as the philosophy of Sam Bankman-Fried. When FTX collapsed and Sam Bankman-Fried was sentenced on seven counts of wire fraud and conspiracy to launder money[^3], I thought that would be the end for this movement. I was wrong.

> The philosophy of effective altruism makes the case that one way that idealistic young people with fancy degrees could improve the world is not by working at nonprofits, but by earning as much money as possible through finance, and then donating it all to vetted causes. [^4]

80,000 Hours advocates for a specific kind of effective altruism called "longtermism"[^6], the idea that "...because so many more humans and other intelligent beings could live in the future than live today, the most important thing for altruistic people to do in the present is to promote the welfare of those unborn beings, by ensuring that future comes to be by preventing existential risks — and that such a future is as good as possible."[^7]

> For various reasons, the [Effective Altruism] movement has turned its attention toward longtermism—a more radical form of its utilitarianism that weighs the value of each future potential life approximately the same as a living person's. Because any human extinction event, however unlikely, imposes infinite costs, longtermists can place enormous moral value on reducing whatever they view as existential risk. [^9]

> [Effective Altruists] initially focused mostly on issues like animal welfare and global poverty, but over time, worries about an AI-fueled apocalypse became a central focus. With funding from deep-pocketed donors like billionaire and Facebook co-founder Dustin Moskovitz, they built their own insular universe to study AI safety, including a web of nonprofits and research organizations, forecasting centers, conferences, and web forums.[^8]

> The most ardent advocates of effective altruism, or EA, believe researchers are only months or years away from building an AI superintelligence able to outsmart the world's collective efforts to control it. Through either its own volition or via terrorists seeking to develop deadly bioweapons, such an AI could wipe out humanity, they say. And some, including noted EA thinker Eliezer Yudkowsky, believe even a nuclear holocaust would be preferable to an unchecked AI future. [^10]

> Make it explicit in international diplomacy that preventing AI extinction scenarios is considered a priority above preventing a full nuclear exchange, and that allied nuclear countries are willing to run some risk of nuclear exchange if that's what it takes to reduce the risk of large AI training runs. [^11]

This to me does not sound like philosophy; this sounds like an AI [doomsday cult](https://en.wikipedia.org/wiki/Doomsday_cult).

> [Effective Altruism]'s career advice center, 80,000 hours, lists "AI safety technical research" and "shaping future governance of AI" as the top two recommended careers for EAs to go into, and the billionaire EA class funds initiatives attempting to stop an AGI apocalypse. According to EAs, AGI is likely inevitable, and their goal is thus to make it beneficial to humanity: akin to creating a benevolent god rather than a devil. [^12]

To borrow a term from Lee Vinsel, this AI apocalypse is another form of criti-hype.[^15] Basically, another [Premature Freak-Out about Technological Enhancement](https://www.scientificamerican.com/article/premature-freak-outs-about-techno-enhancement/).

> Perhaps the two most striking examples of this criti-hype trend are Shoshana Zuboff's book, The Age of Surveillance Capitalism, and the film The Social Dilemma, which includes Zuboff and another criti-hyper Tristan Harris as talking heads. [^15]

Speaking of criti-hyper Tristan Harris[^18], I came across this interview he did with Jon Stewart on The Daily Show.

<iframe src="https://www.youtube.com/embed/675d_6WGPbo" title="Tristan Harris – The Dangers of Unregulated AI on Humanity &amp; the Workforce | The Daily Show" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Tristan Harris, after peddling the idea that social media companies are puppet masters who have users on strings[^15], has now moved onto his next target of AI, falling into the same industry propaganda that he did last time.

This is not to say that there are not any real reasons to be critical of AI. Back in 2020, Robert Julian-Borchak Williams was "wrongfully arrested based on a flawed match from a facial recognition algorithm". [^16] That same year, "two professors and a graduate student had developed a facial-recognition program that could predict whether someone would be a criminal".[^17]

> Advances in data science and machine learning have led to numerous algorithms in recent years that purport to predict crimes or criminality. But if the data used to build those algorithms is biased, the algorithms' predictions will also be biased. Because of the racially skewed nature of policing in the US, the letter argues, any predictive algorithm modeling criminality will only reproduce the biases already reflected in the criminal justice system. [^17]

This is not even to mention the horrifying and harmful firehose of AI slop that is now being generated en masse. [^19][^20][^21][^22][^23][^24][^25][^26]

I think that we need to treat AI as [normal technology](https://knightcolumbia.org/content/ai-as-normal-technology).

> The statement "AI is normal technology" is three things: a description of current AI, a prediction about the foreseeable future of AI, and a prescription about how we should treat it. We view AI as a tool that we can and should remain in control of, and we argue that this goal does not require drastic policy interventions or technical breakthroughs. We do not think that viewing AI as a humanlike intelligence is currently accurate or useful for understanding its societal impacts, nor is it likely to be in our vision of the future. [^13]

> That some significant portion of OpenAI's consumer base is using ChatGPT not so much for the expected "normal" uses like search, or productivity improvements, or creating slop birthday-party invitations, but for friendship, companionship, romance, and therapy certainly feels abnormal. (And apocalyptic.) But this is 2025, and intense, emotional, addiction-resembling attachment to software-bound experience has been a core paradigm of the technology industry for almost two decades, not to mention a multibillion-dollar business model. Certainly, you will not find me arguing that "psychosis-inducing sycophantic girlfriend robot subscription product" is "normal" in the sense of "acceptable" or "appropriate to a mature and dignified civilization." But speaking descriptively, as a matter of long precedent, what could be more normal, in Silicon Valley, than people weeping on a message board because a UX change has transformed the valence of their addiction? [^14]

## Further Reading

- [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)
- [The Artificiality of Alignment](https://joinreboot.org/p/alignment)
- [The Banal Evil of AI Safety](https://www.argmin.net/p/the-banal-evil-of-ai-safety)
- [Artificial Intelligence as Sorcery](https://realizable.substack.com/p/artificial-intelligence-as-sorcery)
- [Magical Thinking on AI](https://aiguide.substack.com/p/magical-thinking-on-ai)
- [Far more authors use AI to write science papers than admit it, publisher reports](https://www.science.org/content/article/far-more-authors-use-ai-write-science-papers-admit-it-publisher-reports)

## References

[^1]: [We're Not Ready for Superintelligence](https://www.youtube.com/watch?v=5KVDDfAkRgc)

[^2]: [The Dev's Creed: Being Wrong is Essential](https://www.youtube.com/watch?v=9PYewjJKD-4)

[^3]: [FTX's Sam Bankman-Fried believed in 'effective altruism'. What is it?](https://www.bbc.com/worklife/article/20231009-ftxs-sam-bankman-fried-believed-in-effective-altruism-what-is-it)

[^4]: [Behind the scenes of how 'effective altruism' went to die in Sam Bankman-Fried's Bahamas penthouse](https://fortune.com/2024/09/02/effective-altruism-sam-bankman-frieds-bahamas-penthouse-ftx-crypto-finance/)

[^5]: [Effective Altruism Is as Bankrupt as Sam Bankman-Fried's FTX](https://www.bloomberg.com/opinion/articles/2023-10-18/effective-altruism-is-as-bankrupt-as-samuel-bankman-fried-s-ftx)

[^6]: [Longtermism: a call to protect future generations](https://80000hours.org/articles/future-generations/)

[^7]: [How effective altruism let Sam Bankman-Fried happen](https://www.vox.com/future-perfect/23500014/effective-altruism-sam-bankman-fried-ftx-crypto)

[^8]: [The AI industry turns against its favorite philosophy](https://www.semafor.com/article/11/21/2023/how-effective-altruism-led-to-a-crisis-at-openai)

[^9]: [The Authoritarian Side of Effective Altruism Comes for AI](https://reason.com/2024/07/05/the-authoritarian-side-of-effective-altruism-comes-for-ai/)

[^10]: [When Silicon Valley's AI warriors came to Washington](https://www.politico.com/news/2023/12/30/ai-debate-culture-clash-dc-silicon-valley-00133323)

[^11]: [Pausing AI Developments Isn't Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/)

[^12]: [Effective Altruism Is Pushing a Dangerous Brand of 'AI Safety'](https://www.wired.com/story/effective-altruism-artificial-intelligence-sam-bankman-fried/)

[^13]: [AI as Normal Technology](https://knightcolumbia.org/content/ai-as-normal-technology)

[^14]: [A.I. as normal technology (derogatory)](https://maxread.substack.com/p/ai-as-normal-technology-derogatory)

[^15]: [You're Doing It Wrong: Notes on Criticism and Technology Hype](https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5)

[^16]: [Wrongfully Accused by an Algorithm](https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html)

[^17]: [An Algorithm That 'Predicts' Criminality Based on a Face Sparks a Furor](https://www.wired.com/story/algorithm-predicts-criminality-based-face-sparks-furor/)

[^18]: [Tristan Harris on the need to change the incentives of social media companies](https://80000hours.org/podcast/episodes/tristan-harris-changing-incentives-social-media/)

[^19]: [Microsoft ignored safety problems with AI image generator, engineer complains](https://www.theguardian.com/technology/2024/mar/06/microsoft-ai-explicit-image-safety)

[^20]: [This is how AI image generators see the world](https://www.washingtonpost.com/technology/interactive/2023/ai-generated-images-bias-racism-sexism-stereotypes/)

[^21]: [4chan Uses Bing to Flood the Internet With Racist Images](https://www.404media.co/4chan-uses-bing-to-flood-the-internet-with-racist-images/)

[^22]: [Where Facebook's AI Slop Comes From](https://www.404media.co/where-facebooks-ai-slop-comes-from/)

[^23]: [AI-generated Nazi memes thrive on Musk's X despite claims of crackdown](https://www.washingtonpost.com/technology/2023/12/14/ai-hate-memes-antisemitic-musk-x/)

[^24]: ['Brainrot' AI on Instagram Is Monetizing the Most Fucked Up Things You Can Imagine (and Lots You Can't)](https://www.404media.co/brainrot-ai-on-instagram-is-monetizing-the-most-fucked-up-things-you-can-imagine-and-lots-you-cant/)

[^25]: [Racist AI-generated videos are the newest slop garnering millions of views on TikTok](https://www.mediamatters.org/tiktok/racist-ai-generated-videos-are-newest-slop-garnering-millions-views-tiktok)

[^26]: ['Just the start': X's new AI software driving online racist abuse, experts warn](https://www.theguardian.com/technology/2025/jan/13/just-the-start-xs-new-ai-software-driving-online-racist-abuse-experts-warn)
